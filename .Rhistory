#Mean Zipf scores per paragraph
Spacemeans=tapply(space_wbN$Zipf, space_wbN$item, mean)
Spacemeans=as.data.frame(Spacemeans)
#Put into Master table
MasterTable$SpaMeanWordFrequency=Spacemeans$Spacemeans
###############################
#Mean Word Length per paragraph
###############################
#######
#Social
#######
soc_wb$wordID=as.character(soc_wb$wordID)
soc_wb$MeanWordLength= nchar(soc_wb$wordID)
socMWL=tapply(soc_wb$MeanWordLength , soc_wb$item, mean)
MasterTable$SocMeanWordLength<-socMWL
########
#Spatial
########
space_wb$wordID=as.character(space_wb$wordID)
space_wb$MeanWordLength= nchar(space_wb$wordID)
spaMWL=tapply(space_wb$MeanWordLength , space_wb$item, mean)
MasterTable$SpaMeanWordLength<-spaMWL
################
# Flesch-Kincaid
################
#Social
###############
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles")
Sockinc=textstat_readability(AllStimSoc$Soc,measure="Flesch.Kincaid")
MasterTable$`SocF-K`<-Sockinc$Flesch.Kincaid
################
# Flesch-Kincaid
################
#Space
################
Spacekinc=textstat_readability(AllStimSpace$Spat ,measure="Flesch.Kincaid")
MasterTable$`SpaF-K`<-Spacekinc$Flesch.Kincaid
#############
#Word Count#
############
#Social
MasterTable$SocNwords <- sapply(AllStimSoc$Soc, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
#Spacial
MasterTable$SpaNwords <- sapply(AllStimSpace$Spat, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
#################
#Check#
View(MasterTable)
##################
#Save Master table
##################
write.csv (MasterTable,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/CompletedMasterTable.csv")
#Remember to check the Master Table to see if it's all there.
setwd("~/Profile/Desktop/SpatSoc Stimuli/SocioSpacial")
write.csv (MasterTable,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/CompletedMasterTable.csv")
Sys.which(git)
Sys.which("git")
write.csv (NambiMaster,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/NambiMasterTable.csv")
write.csv(AmbiMaster,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/AmbiMasterTable.csv")
M=split(MasterTable,MasterTable$Ambiguity)
AmbiMaster=as.data.frame(M$y)
NambiMaster=as.data.frame(M$n)
write.csv (NambiMaster,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/NambiMasterTable.csv")
write.csv(AmbiMaster,"H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/AmbiMasterTable.csv")
write.csv (NambiMaster,"~Output Table/NambiMasterTable.csv")
write.csv(AmbiMaster,"~Output Table/AmbiMasterTable.csv")
write.csv (NambiMaster,"Output Table/NambiMasterTable.csv")
write.csv(AmbiMaster,"Output Table/AmbiMasterTable.csv")
AllStim <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSpace.xlsx")
library("readr")
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
library("readxl")
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
for (i in wbsoc$Item) {
write(wbsoc$word_clean[i], paste0("Word Freeak/TextFiles/SocTxt/NewSoc/Soc",
wbsoc$Item[i], ".txt", sep="" ))}
git(satus)
git config --global user.email "claursen@bournemouth.ac.uk"
setwd("~/Profile/Desktop/NewRSocSpace/SocioSpacial")
install.packages("devtools")
######################################################
#Descriptive Statistics of Spatial and Social Stimuli#
##########################################################################################
#Notes:                                                                                  #
#~Soc stands for Social                                                                  #
#~Spat,Space and maybe some other things stand for Spacial                               #
#~Ambi stands for Ambiguous                                                              #
#~Nambi Stands for Non-Ambiguous                                                         #
#Trickle down changes should be made to the excel files "AllStim","AllstimSoc"and        #
#AllstimSpace or whatever you wanna call them.                                          #
##########################################################################################
#Sorted out code#
#
rm(list= ls())
getwd()
#Packages#
install.packages("wordcloud")
install.packages("tm")
install.packages("arm")
install.packages("MASS")
install.packages("lattice")
install.packages("lme4")
install.packages("effects")
install.packages("SnowballC")
install.packages("RColorBrewer")
install.packages("ngram")
install.packages("quanteda")
install.packages("readr")
#Library#
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("ngram")
library("koRpus")
library("readxl")
library("quanteda")
library("readr")
#Data
########
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
#Master Table
MasterTable <- read_excel("Output Table/MasterTable.xlsx")
###############################
#Text Cleaning and Descriptives
###############################
#Social
#######
wbsoc=AllStimSoc
wbsoc$word_clean<- tolower(wbsoc$Soc)
wbsoc$length<- nchar(wbsoc$Soc)
wbsoc$word_clean<- removePunctuation(wbsoc$word_clean)
wbsoc$Nword <- sapply(wbsoc$word_clean, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
########
#Spatial
########
wbspa=AllStimSpace
wbspa$word_clean<- tolower(wbspa$Spat)
wbspa$length<- nchar(wbspa$Spat)
wbspa$word_clean<- removePunctuation(wbspa$word_clean)
wbspa$Nword <- sapply(wbspa$word_clean, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
###################################################
#Clean Txt file creation for frequency analysis etc
###################################################
#SOcial
#######################
for (i in wbsoc$Item) {
write(wbsoc$word_clean[i], paste0("Word Freeak/TextFiles/SocTxt/NewSoc/Soc",
wbsoc$Item[i], ".txt", sep="" ))}
#Spatial
for(i in wbspa$Item){
write(wbspa$word_clean[i], paste0("Word Freeak/TextFiles/SpaceTxt/NewSPace/Space",
wbspa$Item[i], ".txt", sep="" ))}
####################################################
#Dirty Txt file creation, needed for Flesch.Kincaid
####################################################
#SOcial
#######################
for (i in AllStimSoc$Item) {
write(AllStimSoc$Soc[i], paste0("Word Freeak/TextFiles/OriginalSocial/SocO",
AllStimSoc$Item[i], ".txt", sep="" ))}
###########
#Spatial
###########
for(i in AllStimSoc$Item){
write(AllStimSoc$Soc[i], paste0("Word Freeak/TextFiles/OriginalSpace/Space0",
AllStimSoc$Item[i], ".txt", sep="" ))}
#Not sure what this does but it's needed
get_num<- function(string){as.numeric(unlist(gsub("[^0-9]", "", unlist(string)), ""))}
###############
# File Sort etc
# social:
###############
#setwd("Word Freeak/TextFiles/SocTxt")
soc<- list.files("Word Freeak/TextFiles/SocTxt/NewSoc/")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("Word Freeak/TextFiles/SocTxt/NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
unique(soc_wb$item)
############################
#Zipf analysis from SUBTLEX)
############################
#Social
############################
library(readr)
lex2=read_table2("SUBTLEX-UK/SUBTLEX-UK.txt")
#lex = read_xlsx("//bournemouth.ac.uk/data/staff/home/claursen/Profile/Desktop/SpatSoc Stimuli/SUBTLEX-UK.xlsx")
soc_wb$Zipf<- NA
soc_wb$freq<-NA
for(i in 1:nrow(soc_wb)){
a<- which(lex2$Spelling== soc_wb$wordID[i])
if(length(a)>0){
soc_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
soc_wb$freq[i]<- lex2$FreqCount[a]
}
}
# Check how many and Omit NA values
NASoc=rbind(soc_wb$Zipf==NA,soc_wb$item,soc_wb$wordID)
soc_wbN=na.omit(soc_wb)
#Mean Zipf scores per paragraph
Socmeans=tapply(soc_wbN$Zipf, soc_wbN$item, mean)
Socmeans=as.data.frame(Socmeans)
#Put into Master table
MasterTable$SocMeanWordFrequency=Socmeans$Socmeans
###############
# File Sort etc
# Spatial
###############
get_num<- function(string){as.numeric(unlist(gsub("[^0-9]", "", unlist(string)), ""))}
space<- list.files("Word Freeak/TextFiles/Spacetxt/NewSpace")
n<- get_num(space)
space<- space[order(n, space)]
space<- paste("Word Freeak/TextFiles/Spacetxt/NewSpace/", space, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- space
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
space_wb<- data.frame(item, word_num, wordID)
unique(space_wb$item)
############################
#Zipf analysis from SUBTLEX)
############################
#Spatial
############################
library(readr)
lex2=read_table2("SUBTLEX-UK/SUBTLEX-UK.txt")
#lex = read_xlsx("//bournemouth.ac.uk/data/staff/home/claursen/Profile/Desktop/SpatSoc Stimuli/SUBTLEX-UK.xlsx")
space_wb$Zipf<- NA
space_wb$freq<-NA
for(i in 1:nrow(space_wb)){
a<- which(lex2$Spelling== space_wb$wordID[i])
if(length(a)>0){
space_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
space_wb$freq[i]<- lex2$FreqCount[a]
}
}
# Check how many and Omit NA values
NASoc=rbind(space_wb$Zipf==NA,space_wb$item,space_wb$wordID)
space_wbN=na.omit(space_wb)
#Mean Zipf scores per paragraph
Spacemeans=tapply(space_wbN$Zipf, space_wbN$item, mean)
Spacemeans=as.data.frame(Spacemeans)
#Put into Master table
MasterTable$SpaMeanWordFrequency=Spacemeans$Spacemeans
###############################
#Mean Word Length per paragraph
###############################
#######
#Social
#######
soc_wb$wordID=as.character(soc_wb$wordID)
soc_wb$MeanWordLength= nchar(soc_wb$wordID)
socMWL=tapply(soc_wb$MeanWordLength , soc_wb$item, mean)
MasterTable$SocMeanWordLength<-socMWL
########
#Spatial
########
space_wb$wordID=as.character(space_wb$wordID)
space_wb$MeanWordLength= nchar(space_wb$wordID)
spaMWL=tapply(space_wb$MeanWordLength , space_wb$item, mean)
MasterTable$SpaMeanWordLength<-spaMWL
################
# Flesch-Kincaid
################
#Social
###############
Sockinc=textstat_readability(AllStimSoc$Soc,measure="Flesch.Kincaid")
MasterTable$`SocF-K`<-Sockinc$Flesch.Kincaid
################
# Flesch-Kincaid
################
#Space
################
Spacekinc=textstat_readability(AllStimSpace$Spat ,measure="Flesch.Kincaid")
MasterTable$`SpaF-K`<-Spacekinc$Flesch.Kincaid
#############
#Word Count#
############
#Social
MasterTable$SocNwords <- sapply(AllStimSoc$Soc, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
#Spacial
MasterTable$SpaNwords <- sapply(AllStimSpace$Spat, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
#################
#Check#
View(MasterTable)
##################
#Save Master table
##################
write.csv (MasterTable,"Output Table/CompletedMasterTable.csv")
#Remember to check the Master Table to see if it's all there.
##############
#Reading Ease#
#For curiosity#
##############
#FTSo=textstat_readability(AllStimSoc$Soc ,measure="Flesch")
#FTSp=textstat_readability(AllStimSpace$Spat ,measure="Flesch")
#MasterTable$ReadingeaseSoc=FTSo$Flesch
#MasterTable$ReadingeaseSpat=FTSp$Flesch
install.packages(c("koRpus", "readxl"))
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
library("readr")
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
library("readxl")
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
AmbiSpace= split.data.frame(AllStimSpace,AllStimSpace$Ambi)
NambiSpace=AmbiSpace$n
AmbiSpace=AmbiSpace$y
AmbiSoc= split.data.frame(AllStimSoc,AllStimSoc$Ambi)
NambiSoc=AmbiSoc$n
AmbiSoc=AmbiSoc$y
NambiSpace$Item=c(1:24)
NambiSoc$Item=c(1:24)
View(AmbiSoc)
View(AmbiSpace)
library(readxl)
dataset <- read_excel(NULL)
View(dataset)
Cond1=read_xlsx("CounterB/Conditions/Cond1.xlsx")
View(Cond1)
Cond1$Stimulus=Cond1$P1
View(Cond1)
Cond1$P1=AmbiSoc$Soc
Cond1$P2=AmbiSpace$Spat
View(Cond1)
install.packages("xlsx")
library("xlsx")
write.xlsx(Cond1, "CounterB/Conditions/Cond1.xlsx", sheetName = "Sheet1",
col.names = TRUE, row.names = TRUE, append = FALSE)
library("xlsx")
write.xlsx(Cond1, "CounterB/Conditions/Cond1.xlsx", sheetName = "Sheet1",
col.names = TRUE, row.names = TRUE, append = FALSE)
write.csv(Cond1, "CounterB/Conditions/Cond1.xlsx", sheetName = "Sheet1",
col.names = TRUE, row.names = TRUE, append = FALSE)
install.packages("xlsx")
library("xlsx")
write.table(Cond1, "CounterB/Conditions/Cond1.xlsx",
col.names = TRUE, row.names = TRUE, append = FALSE)
write.table(Cond1, "CounterB/Conditions/Cond1.csv",
col.names = TRUE, row.names = TRUE, append = FALSE)
Cond1=read.csv("CounterB/Conditions/Cond1.csv")
View(Cond1)
for (i in Cond1$Item.Cond.P1.P2){
write(Cond1$Item.Cond.P1.P2[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Item.Cond.P1.P2[i],".txt",sep=""))}
Cond1=as.data.frame(Cond1)
for (i in Cond1$Item.Cond.P1.P2){
write(Cond1$Item.Cond.P1.P2[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Item.Cond.P1.P2[i],".txt",sep=""))}
Cond1$Stimulus=Cond1$Item.Cond.P1.P2
Cond1$Item=c(1:24)
View(Cond1)
for (i in Cond1$Item){
write(Cond1$Stimulus[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Item[i],".txt",sep=""))}
for (i in Cond1$Item){
write(Cond1$Stimulus[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Stimulus[i],".txt",sep=""))}
for (i in Cond1$Item){
write(Cond1$Stimulus[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Item[i],".txt",sep=""))}
Cond1=read_xlsx("CounterB/Conditions/Cond1.xlsx")
Cond1=read_xlsx("CounterB/Conditions/Cond2.xlsx")
install.packages("wordcloud")
install.packages("tm")
install.packages("arm")
install.packages("MASS")
install.packages("lattice")
install.packages("lme4")
install.packages("effects")
install.packages("SnowballC")
install.packages("RColorBrewer")
install.packages("ngram")
install.packages("quanteda")
install.packages("readr")
#Library#
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("ngram")
library("koRpus")
library("readxl")
library("quanteda")
library("readr")
Cond1=read_xlsx("CounterB/Conditions/Cond2.xlsx")
View(Cond1)
AllStim <- read_excel("Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("Stimuli/AllStimSpace.xlsx")
AmbiSpace= split.data.frame(AllStimSpace,AllStimSpace$Ambi)
NambiSpace=AmbiSpace$n
AmbiSpace=AmbiSpace$y
AmbiSoc= split.data.frame(AllStimSoc,AllStimSoc$Ambi)
NambiSoc=AmbiSoc$n
AmbiSoc=AmbiSoc$y
NambiSpace$Item=c(1:24)
NambiSoc$Item=c(1:24)
View(Cond1)
Cond1$Cond=c(1)
View(Cond1)
Cond1$Stimulus=c(AmbiSoc$Soc,AmbiSpace$Spat)
Cond1$Stimulus=cbind(AmbiSoc$Soc,AmbiSpace$Spat)
View(Cond1)
for (i in Cond1$Item){
write(Cond1$Stimulus[i],paste0("CounterB/Conditions/Cond1txt/Cond1",
Cond1$Item[i],".txt",sep=""))}
Cond1$Stimulus=unite(AmbiSoc$Soc,AmbiSpace$Spat)
install.packages("tidyr")
library("tidyr")
Cond1$Stimulus=unite(AmbiSoc$Soc,AmbiSpace$Spat)
Cond1$Stimulus=merge(AmbiSoc$Soc,AmbiSpace$Spat,FUN=paste)
Cond1$Stimulus=c(AmbiSoc$Soc,AmbiSpace$Spat,FUN=paste)
Cond1$Stimulus=cbind(AmbiSoc$Soc,AmbiSpace$Spat,FUN=paste)
View(Cond1)
Cond1$Stimulus=rbind(AmbiSoc$Soc,AmbiSpace$Spat,FUN=paste)
Cond1$Stimulus=rbind(AmbiSoc$Soc,AmbiSpace$Spat)
Cond1$Stimulus=as.vector(rbind(AmbiSoc$Soc,AmbiSpace$Spat))
Cond1$Stimulus=as.character.vector(rbind(AmbiSoc$Soc,AmbiSpace$Spat))
Cond1$Stimulus=as.character.vector(cbind(AmbiSoc$Soc,AmbiSpace$Spat))
Cond1$Stimulus=as.vector(cbind(AmbiSoc$Soc,AmbiSpace$Spat))
Cond1$Stimulus=cbind(AmbiSoc$Soc,AmbiSpace$Spat)
View(Cond1)
write.csv(Cond1,"CounterB/Conditions/tst.csv",Header=TRUE,col.names = TRUE)
write.csv(Cond1,"CounterB/Conditions/tst.csv",col.names = TRUE)
AllQuest <- read_excel("Stimuli/Questions.xlsx")
install.packages("wordcloud")
install.packages("tm")
install.packages("arm")
install.packages("MASS")
install.packages("lattice")
install.packages("lme4")
install.packages("effects")
install.packages("SnowballC")
install.packages("RColorBrewer")
install.packages("ngram")
install.packages("quanteda")
install.packages("readr")
install.packages("tidyr")
#Library#
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("ngram")
library("koRpus")
library("readxl")
library("quanteda")
library("readr")
library("tidyr")
AllQuest <- read_excel("Stimuli/Questions.xlsx")
View(AllQuest)
AllQuest$SoCleanQ=NULL
AllQuest$SpCleanQ=NULL
AllQuest$SoCleanQ=tolower(AllQuest$`Social Question`)
AllQuest$SpCleanQ=tolower(AllQuest$`Spatial Question`)
AllQuest$SoCleanQ=removePunctuation(AllQuest$`Social Question`)
AllQuest$SpCleanQ=removePunctuation(AllQuest$`Spatial Question`)
AllQuest$SocQNW=sapply(AllQuest$SoCleanQ, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
AllQuest$SpaQNW=sapply(AllQuest$SpCleanQ, function(x) length(unlist(strsplit(as.character(x), "\\W+"))
SocQkinc=textstat_readability(AllQuest$`Social Question`,measure="Flesch.Kincaid")
AllQuest$SocQKinc<-Sockinc$Flesch.Kincaid
SpaQkinc=textstat_readability(AllQuest$`Spatial Question`,measure="Flesch.Kincaid")
AllQuest$SpaKinc<-SpaQkinc$Flesch.Kincaid
View(AllQuest)
AllQuest$SpaQNW=sapply(AllQuest$SpCleanQ, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
View(AllQuest)
FCSoc=mean(AllQuest$SocQKinc)
FCSpa=mean(AllQuest$SpaKinc)
NWSocM=mean(AllQuest$SocQNW)
NWSpaM=mean(AllQuest$SpaQNW)
View(AllQuest)
View(MasterTable)
MeanSOCNW=mean(MasterTable$SocNwords)
mean(MasterTable$SocNwords)
unique(MasterTable$Ambiguity)
unique(MasterTable)
