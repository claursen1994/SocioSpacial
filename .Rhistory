word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
View(soc_wb)
View(soc_wb)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("ngram")
library("koRpus")
library("readxl")
library("quanteda")
library("readr")
lex2=read_table2("//bournemouth.ac.uk/data/staff/home/claursen/Profile/Desktop/SpatSoc Stimuli/SUBTLEX-UK.txt")
lex2=read_table2("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/SUBTLEX-UK/SUBTLEX-UK.txt")
soc_wb$Zipf<- NA
soc_wb$freq<-NA
for(i in 1:nrow(soc_wb)){
a<- which(lex$Spelling== soc_wb$wordID[i])
if(length(a)>0){
soc_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
soc_wb$freq[i]<- lex2$FreqCount[a]
}
}
soc_wb$Zipf<- NA
soc_wb$freq<-NA
for(i in 1:nrow(soc_wb)){
a<- which(lex2$Spelling== soc_wb$wordID[i])
if(length(a)>0){
soc_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
soc_wb$freq[i]<- lex2$FreqCount[a]
}
}
View(soc_wb)
unique(soc_wb$item)
# social:
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
# social:
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/SocTxt")
get_num<- function(string){as.numeric(unlist(gsub("[^0-9]", "", unlist(string)), ""))}
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
unique(soc_wb$item)
rm(list= ls())
getwd()
AllStim <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSpace.xlsx")
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("ngram")
library("koRpus")
library("readxl")
library("quanteda")
library("readr")
AllStim <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStim.xlsx")
AllStimSoc <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSoc.xlsx")
AllStimSpace <- read_excel("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Stimuli/AllStimSpace.xlsx")
wbsoc=AllStimSoc
wbsoc$word_clean<- tolower(wbsoc$Soc)
wbsoc$length<- nchar(wbsoc$Soc)
wbsoc$word_clean<- removePunctuation(wbsoc$word_clean)
wbsoc$Nword <- sapply(wbsoc$word_clean, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
wbspa=AllStimSpace
wbspa$word_clean<- tolower(wbspa$Spat)
wbspa$length<- nchar(wbspa$Spat)
wbspa$word_clean<- removePunctuation(wbspa$word_clean)
wbspa$Nword <- sapply(wbspa$word_clean, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
for (i in wbsoc$Item) {
write(wbsoc$word_clean[i], paste0("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/SocTxt/NewSoc/Soc",
wbsoc$Item[i], ".txt", sep="" ))}
for(i in wbspa$Item){
write(wbspa$word_clean[i], paste0("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/SpaceTxt/NewSPace/Space",
wbspa$Item[i], ".txt", sep="" ))}
get_num<- function(string){as.numeric(unlist(gsub("[^0-9]", "", unlist(string)), ""))}
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/SocTxt")
soc<- list.files("NewSoc")
n<- get_num(soc)
soc<- soc[order(n, soc)]
soc<- paste("NewSoc/", soc, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- soc
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
unique(soc_wb$item)
View(soc_wb)
for (p in unique(soc_wb$item)){
Para[p]<-soc_wb$wordID
}
for (p in unique(soc_wb$item)){
Para[p]<-soc_wb$wordID
}
library(readr)
lex2=read_table2("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/SUBTLEX-UK/SUBTLEX-UK.txt")
lex = read_xlsx("//bournemouth.ac.uk/data/staff/home/claursen/Profile/Desktop/SpatSoc Stimuli/SUBTLEX-UK.xlsx")
soc_wb$Zipf<- NA
soc_wb$freq<-NA
for(i in 1:nrow(soc_wb)){
a<- which(lex2$Spelling== soc_wb$wordID[i])
if(length(a)>0){
soc_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
soc_wb$freq[i]<- lex2$FreqCount[a]
}
}
View(soc_wb)
split(soc_wb, soc_wb$item)
Worba=split(soc_wb, soc_wb$item)
Worba=as.data.frame(Worba)
Worba=c(soc_wb$item,soc_wb$Zipf)
Worba=as.data.frame(Worba)
View(Worba)
Worba$Items=soc_wb$item
Worba$Zipf=soc_wb$Zipf
View(Worba)
split(Worba$Items)
split(Worba, Worba$Items)
Werb=c(split(Worba, Worba$Items))
L=unique(Worba$Items)
for L in 1:nrow(Worba$Items) {
mean(Worba$Zipf)
}
Worba$Items=soc_wb$item==1
View(Worba)
Werb=c(split(Worba, Worba$Items,header=TRUE))
Omeg=Werb$Zipf
write_excel_csv(Werb)
Werb1=Werb$`1`
View(Werb1)
Werb1=mean(Werb1$Zipf)
Werb1=Werb$`1`
write_excel_csv(Werb1)
werb=c(split(soc_wb, soc_wb$item))
Werb=c(split(soc_wb, soc_wb$item))
for (i in Werb){
WerbD[i]=Werb[i]
}
View(Werb1)
i=48
for (i in Werb){
WerbD[i]=Werb[i]
}
for (i in Werb$`i`){
WerbD[i]=Werb[i]
}
lapply(Werb,mean)
Socmeans<- sapply(soc_wb$item, function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
Socmeans<- sapply(soc_wb$item,soc_wb$Zipf function(x) length(unlist(strsplit(as.character(x), "\\W+"))))
soc_wb$Means <-
with (soc_wb, ave( item, findInterval(item, c(-Inf, 1, 1, Inf)), FUN= mean) )
View(soc_wb)
soc_wb$Means <-
with (soc_wb, ave( item, findInterval(item, c(-Inf, Inf)), FUN= mean) )
View(soc_wb)
aggregate(.~item, FUN=mean, Zipf=soc_wb[, -2])
aggregate(.~names(item), FUN=mean, Zipf=soc_wb[, -2])
soc_wb$means=tapply(soc_wb$Zipf, soc_wb$item, mean)
Socmeans=tapply(soc_wb$Zipf, soc_wb$item, mean)
view(Socmeans)
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
soc_wb<- data.frame(item, word_num, wordID)
View(soc_wb)
for(i in 1:nrow(soc_wb)){
if(substr(soc_wb$wordID[i], 1, 1)== "'"){
soc_wb$wordID[i]<- substr(soc_wb$wordID[i], 2, nchar(soc_wb$wordID[i]))
}
if(is.element(substr(soc_wb$wordID[i], nchar(soc_wb$wordID[i]), nchar(soc_wb$wordID[i])), c("'", ",", ".", "!", ";", ":"))){
soc_wb$wordID[i]<- substr(soc_wb$wordID[i], 1, nchar(soc_wb$wordID[i])-1)
}
if(is.element(substr(soc_wb$wordID[i], nchar(soc_wb$wordID[i]), nchar(soc_wb$wordID[i])), c(",", ".", "!"))){
soc_wb$wordID[i]<- substr(soc_wb$wordID[i], 1, nchar(soc_wb$wordID[i])-1)
}
if(!is.element(soc_wb$wordID[i], c("I", "I'm"))){
soc_wb$wordID[i]<- tolower(soc_wb$wordID[i])
}
}
Socmeans=as.data.frame(Socmeans)
View(Socmeans)
View(wbsoc)
View(soc_wb)
soc_wb$Zipf<- NA
soc_wb$freq<-NA
for(i in 1:nrow(soc_wb)){
a<- which(lex2$Spelling== soc_wb$wordID[i])
if(length(a)>0){
soc_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
soc_wb$freq[i]<- lex2$FreqCount[a]
}
}
View(soc_wb)
View(soc_wb)
NASoc=soc_wb$Zipf==NA
NASoc=as.data.frame(NASoc)
View(NASoc)
NASoc=c(soc_wb$Zipf==NA,soc_wb$item,soc_wb$wordID)
NASoc=as.data.frame(NASoc)
View(NASoc)
NASoc=cbind(soc_wb$Zipf==NA,soc_wb$item,soc_wb$wordID)
View(NASoc)
NASoc=cbind(soc_wb$Zipf==NA,soc_wb$item,soc_wb$wordID, Header=TRUE)
View(NASoc)
NASoc=rbind(soc_wb$Zipf==NA,soc_wb$item,soc_wb$wordID)
View(NASoc)
soc_wb$Zipf=na.omit(soc_wb$Zipf)
soc_wb$Zipf=na.exclude(soc_wb$Zipf)
soc_wbN=na.omit(soc_wb)
View(soc_wbN)
Socmeans=tapply(soc_wbN$Zipf, soc_wbN$item, mean)
Socmeans=as.data.frame(Socmeans)
View(Socmeans)
View(Socmeans)
library(readxl)
MasterTable <- read_excel("~/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Output Table/MasterTable.xlsx")
View(MasterTable)
View(Socmeans)
MasterTable$SocMeanWordFrequency=Socmeans$Socmeans
View(MasterTable)
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/Spacetxt")
get_num<- function(string){as.numeric(unlist(gsub("[^0-9]", "", unlist(string)), ""))}
space<- list.files("NewSpace")
n<- get_num(space)
space<- space[order(n, space)]
space<- paste("NewSpace/", space, sep= '')
item<- NULL
word_num<- NULL
curr_item= NULL
wordID= NULL
files<- space
for(i in 1:length(files)){ # for each text page..
text<- readLines(files[i]) # read in text
string<- unlist(strsplit(text, " ")) # split by word
curr_item<- get_num(files[i]) # item (file #)
item<- c(item, rep(curr_item, length(string)))
word_num<- c(word_num, 1:length(string)) # word number in each text
wordID<- c(wordID, string)
}
space_wb<- data.frame(item, word_num, wordID)
unique(space_wb$item)
View(space_wb)
space_wb$Zipf<- NA
space_wb$freq<-NA
for(i in 1:nrow(space_wb)){
a<- which(lex2$Spelling== space_wb$wordID[i])
if(length(a)>0){
space_wb$Zipf[i]<- lex2$`LogFreq(Zipf)`[a]
space_wb$freq[i]<- lex2$FreqCount[a]
}
}
# Check how many and Omit NA values
NASoc=rbind(space_wb$Zipf==NA,space_wb$item,space_wb$wordID)
space_wbN=na.omit(space_wb)
#Mean Zipf scores per paragraph
Spacemeans=tapply(space_wbN$Zipf, space_wbN$item, mean)
Spacemeans=as.data.frame(Spacemeans)
#Put into Master table
MasterTable$SpaMeanWordFrequency=Spacemeans$Spacemeans
View(soc_wbN)
View(MasterTable)
View(soc_wbN)
View(soc_wb)
soc_wb$MeanWordLength= nchar(soc_wb$wordID)
soc_wb$wordID=as.character(soc_wb$wordID)
View(soc_wb)
soc_wb$MeanWordLength= nchar(soc_wb$wordID)
View(soc_wb)
socMWL=tapply(soc_wb$MeanWordLength , space_wbN$item, mean)
socMWL=tapply(soc_wb$MeanWordLength , soc_wbN$item, mean)
socMWL=tapply(soc_wb$MeanWordLength , soc_wb$item, mean)
SocMWL=as.data.frame(SocMWL)
socMWL=tapply(soc_wb$MeanWordLength , soc_wb$item, mean)
SocMWL=as.data.frame(SocMWL)
MasterTable$SocMeanWordLength<-socMWL
View(MasterTable)
space_wb$wordID=as.character(space_wb$wordID)
space_wb$MeanWordLength= nchar(space_wb$wordID)
spaMWL=tapply(space_wb$MeanWordLength , space_wb$item, mean)
MasterTable$SocMeanWordLength<-spaMWL
View(MasterTable)
soc_wb$wordID=as.character(soc_wb$wordID)
soc_wb$MeanWordLength= nchar(soc_wb$wordID)
socMWL=tapply(soc_wb$MeanWordLength , soc_wb$item, mean)
MasterTable$SocMeanWordLength<-socMWL
MasterTable$SpaMeanWordLength<-spaMWL
View(MasterTable)
for(i in 1:length(files)){ # for each text page..
textk<- readLines(files[i]) # read in text
Spacekincaid=textk[i]
}
for(i in 1:length(files)){ # for each text page..
t<- readLines(files[i]) # read in text
t2=textstat_readability(x=t[i], measure = "Flesch.Kincaid")
}
View(AllStimSpace)
t=readLines(AllStimSpace$Spat)
MasterTable$`SpaF-K`=textstat_readability(x = t, measure = "Flesch.Kincaid")
t=readLines((nrow)AllStimSpace$Spat)
for (i in wbsoc$Item) {
textstat_readability(x=wbsoc$word_clean[i], measure = "Flesch.Kincaid", paste0("MasterTable$`SpaF-K`"
))}
for (i in wbsoc$Item) {
((textstat_readability(x=wbsoc$word_clean[i], measure = "Flesch.Kincaid"), paste0("MasterTable$`SpaF-K`"
)))}
for(i in 1:length(files)){ # for each text page..
t<- readLines(files[i]) # read in text
t2=textstat_readability(x=t[i], measure = "Flesch.Kincaid")
}
t=readLines("Space1.txt")
t2=textstat_readability(x=t, measure = "Flesch-Kincaid")
t=readLines("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/Spacetxt/NewSpace/Space1.txt")
t2=textstat_readability(x=t, measure = "Flesch-Kincaid")
t=readLines("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/Spacetxt/NewSpace/Space1.txt")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
t=readLines("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/Spacetxt/Space.txt1.txt")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
View(t2)
Orisoc<- list.files("OriginalSocial")
n<- get_num(Orisoc)
Orisoc<- Orisoc[order(n, Orisoc)]
Orisoc<- paste("OriginalSocial/", Orisoc, sep= '')
t=readLines("Orisoc")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
t=readLines("OriginalSocial/")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
t=readLines("Orisoc")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
Orisoc<- list.files("OriginalSocial")
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles/Spacetxt")
setwd("H:/Profile/Desktop/SpatSoc Stimuli/SocioSpacial/Word Freeak/TextFiles")
Orisoc<- list.files("OriginalSocial")
n<- get_num(Orisoc)
Orisoc<- Orisoc[order(n, Orisoc)]
Orisoc<- paste("OriginalSocial/", Orisoc, sep= '')
t=readLines("Orisoc")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
t=readLines("Orisoc/")
t2=textstat_readability(x=t, measure = "Flesch.Kincaid")
for(i in 1:length(Orisoc)){ # for each text page..
text<- readLines(Orisoc[i]) # read in text
t2=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
View(t2)
t=readLines("Orisoc/")
for(i in 1:length(Orisoc)){ # for each text page..
text<- readLines(Orisoc[i]) # read in text
t2=c(textstat_readability(x=text[i], measure = "Flesch.Kincaid"))
}
for(i in 1:length(Orisoc)){ # for each text page..
text<- readLines(Orisoc[i]) # read in text
MasterTable$`SocF-K`[i]=c(textstat_readability(x=text[i], measure = "Flesch.Kincaid"))
}
View(MasterTable)
for(i in 1:length(Orisoc)){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
MasterTable$`SocF-K`[i]=c(textstat_readability(x=text[i], measure = "Flesch.Kincaid"))
}
View(MasterTable)
for(i in 1:length(Orisoc)){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
MasterTable$`SocF-K`[i]=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
View(MasterTable)
for(i in 1:length(Orisoc)){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
for(i in 1:length(Orisoc)){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
Kincaid=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
View(Kincaid)
for(i in 1:length(Orisoc)){ # for each text page..
text[i]<- readLines(Orisoc) # read in text
Kincaid=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
for(i in 1:length(Orisoc[i])){ # for each text page..
text[i]<- readLines(Orisoc) # read in text
Kincaid$Flesch.Kincaid[i]=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
for(i in 1:length(Orisoc[i])){ # for each text page..
text[i]<- readLines(Orisoc) # read in text
Kincaid$Flesch.Kincaid=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
for(i in 1:length(Orisoc[i])){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
Kincaid$Flesch.Kincaid=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
for(i in 1:length(Orisoc[i])){ # for each text page..
text[i]<- readLines(Orisoc[i]) # read in text
Kincaid$Flesch.Kincaid[i]=textstat_readability(x=text[i], measure = "Flesch.Kincaid")
}
rm(list= ls())
